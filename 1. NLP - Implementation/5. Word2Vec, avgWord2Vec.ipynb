{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e88c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install nltk\n",
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4679c415",
   "metadata": {},
   "source": [
    "## **Spam or Ham Classifier Project**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e5ca19",
   "metadata": {},
   "source": [
    "### **Importing the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fbae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "messages = pd.read_csv(\n",
    "    '../NLP-for-Transformers/Datum/SMSSpamCollection',\n",
    "    sep='\\t',\n",
    "    names=['label', 'message']\n",
    ")\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3a9045",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(messages['message'],'\\n')\n",
    "\n",
    "print(messages['message'].loc[100],'\\n')\n",
    "print(messages['message'].loc[451])\n",
    "\n",
    "'''\n",
    "1. messages refers to the pandas DataFrame that contains the dataset.\n",
    "\n",
    "2. ['message'] selects the \"message\" column from the DataFrame,\n",
    "   which contains the SMS/text content.\n",
    "\n",
    "3. .loc[451] selects the row with index label 451.\n",
    "\n",
    "4. This returns the text message present at index 451 in the dataset.\n",
    "\n",
    "5. It is commonly used to inspect or view a specific sample message\n",
    "   from the corpus for understanding or debugging.\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381ea8ef",
   "metadata": {},
   "source": [
    "### **Step-1: Text Preprocessing** \n",
    "    1. Tokenization, \n",
    "    2. StopWords, \n",
    "    3. Stemmming, \n",
    "    4. Lemmatization, \n",
    "    5. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96864c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and preprocessing\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b8a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer # See the Capital letters\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f49c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for i in range(len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['message'][i])\n",
    "    review = review.lower().split()\n",
    "\n",
    "    review1 = []\n",
    "    for word in review:\n",
    "        if word not in stop_words:\n",
    "            review1.append(ps.stem(word))\n",
    "\n",
    "    review = ' '.join(review1)\n",
    "    corpus.append(review)\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f0852",
   "metadata": {},
   "source": [
    "### **Step-2: Text --> Vectors** \n",
    "    1.BoW, \n",
    "    2. TF-IDF, \n",
    "    3. Word2Vec, \n",
    "    4. AvgWord2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f29f20e",
   "metadata": {},
   "source": [
    "### **Creating the Bag of Words model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ae8e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features = 2500)\n",
    "x = cv.fit_transform(corpus)\n",
    "\n",
    "x = x.toarray()\n",
    "\n",
    "'''\n",
    "1. These below lines implement the Bag of Words (BoW) model on your text data.\n",
    "\n",
    "2. This creates a CountVectorizer object.\n",
    "    What it does:\n",
    "    Converts text â†’ numeric vectors\n",
    "    Builds a vocabulary from your corpus\n",
    "    Represents each document by word counts\n",
    "\n",
    "3. Keeps only the top 2500 most frequent words in the corpus.\n",
    "\n",
    "4. x = cv.fit_transform(corpus)\n",
    "This does two things:\n",
    "    ðŸ”¹ fit(corpus)\n",
    "        Scans all documents in corpus\n",
    "        Learns the vocabulary (up to 2500 words)\n",
    "        Assigns each word an index (column)\n",
    "\n",
    "    ðŸ”¹ transform(corpus)\n",
    "        Converts each document into a vector\n",
    "        Each vector length = vocabulary size (â‰¤ 2500)\n",
    "        Values = word counts in that document\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dbd8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(messages['label'])\n",
    "y = y.iloc[:,1].values\n",
    "\n",
    "'''\n",
    "1. pd.get_dummies(messages['label']) converts the text labels (like \"ham\" and \"spam\")\n",
    "   into separate binary columns (one-hot encoding).\n",
    "\n",
    "2. Each label becomes a column with values 0 or 1.\n",
    "   Example:\n",
    "      \"ham\"  â†’ [1, 0]\n",
    "      \"spam\" â†’ [0, 1]\n",
    "\n",
    "3. y.iloc[:, 1] selects the second column from the dummy DataFrame,\n",
    "   which usually corresponds to the \"spam\" class.\n",
    "\n",
    "4. .values converts the selected column into a NumPy array.\n",
    "\n",
    "5. The final target vector y becomes:\n",
    "      spam â†’ 1\n",
    "      ham  â†’ 0\n",
    "\n",
    "6. This numeric y is used as the output/label variable\n",
    "   for training the machine learning classification model.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cddff60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83349277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1db8db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8952ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4f8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4877f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ff40c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b0cfec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd760a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb4d8bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
