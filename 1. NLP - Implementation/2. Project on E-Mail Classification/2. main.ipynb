{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "372c9ed6",
   "metadata": {},
   "source": [
    "### Data Collection..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db55392c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "messages = pd.read_csv(\n",
    "    '../NLP-for-Transformers/Datum/SMSSpamCollection',\n",
    "    # sep='\\t',\n",
    "    # names=['label', 'message']\n",
    ")\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6f7001",
   "metadata": {},
   "source": [
    "### Data Preparation and preprocessing..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb1edf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer # See the Capital letters\n",
    "\n",
    "stop_words = set(stopwords.words('english')) # List to Set\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2cedcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [] # Group of Sentences\n",
    "for i in range(len(messages)): # for each sentence in that message\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['message'][i])\n",
    "    review = review.lower().split()\n",
    "\n",
    "    cleaned_words = [] # Group of Individual Words\n",
    "    for word in review: # for each word in that sentence\n",
    "        if word not in stop_words:\n",
    "            s_word = ps.stem(word)\n",
    "            cleaned_words.append(s_word) # Adds the Stemmed word to the Sentence List\n",
    "\n",
    "    Sentence = ' '.join(cleaned_words) # Forming the sentance from Cleaned Words\n",
    "    corpus.append(Sentence) # Adds the cleaned sentence to the corpus List \n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd0ccc",
   "metadata": {},
   "source": [
    "#### Applying OHE for the y lable: (Creating y Here..!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20657ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(messages['label']) # Dummies --> OHE.\n",
    "y = y.iloc[:,1] # selecting the 2nd column of the spam,ham\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be96a3c9",
   "metadata": {},
   "source": [
    "### **Step-2: Text --> Vectors** (Creating X Here..!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759052c6",
   "metadata": {},
   "source": [
    "### **1. Creating the Bag Of Words Model for Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87204e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Using CountVectorizer for BoW Vectors\n",
    "cv = CountVectorizer(max_features = 2500) # Implement the Bag of Words\n",
    "# cv = CountVectorizer(max_features = 2500, binary = True, ngram_range = (1,2))\n",
    "X = cv.fit_transform(corpus)\n",
    "\n",
    "X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a837bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3183651e",
   "metadata": {},
   "source": [
    "#### **MultinomialNB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc030d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Object for the model class..!\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_detect_model = MultinomialNB()\n",
    "spam_detect_model.fit(X_train, y_train)\n",
    "\n",
    "# Prediction\n",
    "\n",
    "y_pred = spam_detect_model.predict(X_test)\n",
    "\n",
    "# Accuracy Metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "accuracyScore = accuracy_score(y_test, y_pred)\n",
    "print(accuracyScore)\n",
    "classy_report = classification_report(y_test, y_pred)\n",
    "print(classy_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d2ee55",
   "metadata": {},
   "source": [
    "#### **RandomForestClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d1a95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Object for the model class..!\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train) # not fit_transform\n",
    "\n",
    "# Predicting\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(acc)\n",
    "clss = classification_report(y_test, y_pred)\n",
    "print(clss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4342c4",
   "metadata": {},
   "source": [
    "### **2. Creating the TF-IDF Model for Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8512822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Using TfidfVectorizer for TF-IDF Vectors\n",
    "tv = TfidfVectorizer(max_features = 2500)\n",
    "# tv = TfidfVectorizer(max_features = 2500, ngram_range = (1,2))\n",
    "X = tv.fit_transform(corpus)\n",
    "X.toarray()\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbb0a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5625c85",
   "metadata": {},
   "source": [
    "#### **MultinomialNB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc144529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Object for the model class..!\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_detect_model = MultinomialNB()\n",
    "spam_detect_model.fit(X_train, y_train)\n",
    "\n",
    "# prediction\n",
    "\n",
    "y_pred = spam_detect_model.predict(X_test)\n",
    "\n",
    "# Accuracy Metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "accuracyScore = accuracy_score(y_test,y_pred)\n",
    "print(accuracyScore)\n",
    "classy_report = classification_report(y_test,y_pred)\n",
    "print(classy_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7310e60a",
   "metadata": {},
   "source": [
    "#### **RandomForestClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa3ba86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Object for the model class..!\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train) # not fit_transform\n",
    "\n",
    "# Predicting\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(acc)\n",
    "clss = classification_report(y_test, y_pred)\n",
    "print(clss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca7973",
   "metadata": {},
   "source": [
    "### **3. Creating the Word2Vec Model for Vectorization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3c4343",
   "metadata": {},
   "source": [
    "### **Loading the data or Data Collection..!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42864102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dc2f9a",
   "metadata": {},
   "source": [
    "### Data Preprocesssing and preparation..!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc1c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "from gensim.utils import simple_preprocess # simple_preprocess will simply convert to lowercase..!\n",
    "\n",
    "words=[]\n",
    "for sent in corpus:\n",
    "    sent_token = sent_tokenize(sent)\n",
    "    for sent in sent_token:\n",
    "        words.append(simple_preprocess(sent))\n",
    "\n",
    "print(len(words))\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d24e846",
   "metadata": {},
   "source": [
    "#### **# Creating Object for the model class..!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34303752",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(words,window=5,min_count=2) # Here Vector_size=100 and epochs=5 are default values..!\n",
    "model.wv.index_to_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19038ec7",
   "metadata": {},
   "source": [
    "### **4. Creating the AvgWord2Vec Model for Vectorization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ffe58e",
   "metadata": {},
   "source": [
    "#### **Defining the function for avg_word2vec**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5538a9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def avg_word2vec(doc):\n",
    "    \"\"\"\n",
    "    Compute the average Word2Vec embedding for a document.\n",
    "    doc: list of tokens (words)\n",
    "    \"\"\"\n",
    "    \n",
    "    vectors = []  # to store word vectors found in the model\n",
    "    \n",
    "    for word in doc:\n",
    "        # check if word exists in Word2Vec vocabulary\n",
    "        if word in model.wv.index_to_key:\n",
    "            vectors.append(model.wv[word])\n",
    "    \n",
    "    # if no words from doc are in vocabulary, return a zero vector\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    # compute and return the mean vector\n",
    "    avg_vector = np.mean(vectors, axis=0)\n",
    "    return avg_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf5487",
   "metadata": {},
   "source": [
    "#### **Calling the avg_word2vec function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f02c4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply for the entire sentences\n",
    "\n",
    "X=[]\n",
    "for i in tqdm(range(len(words))):\n",
    "    # print(\"Hello\",i)\n",
    "    X.append(avg_word2vec(words[i]))\n",
    "\n",
    "X.shape\n",
    "\n",
    "X_new = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01982315",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train Test Split and Apply a Model to this..!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
