{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e88c138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install nltk\n",
    "# !pip install scikit-learn\n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4679c415",
   "metadata": {},
   "source": [
    "## **Spam or Ham Classifier Project**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e5ca19",
   "metadata": {},
   "source": [
    "### **Importing the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fbae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "messages = pd.read_csv(\n",
    "    '../NLP-for-Transformers/Datum/SMSSpamCollection',\n",
    "    sep='\\t',\n",
    "    names=['label', 'message']\n",
    ")\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3a9045",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(messages['message'],'\\n')\n",
    "\n",
    "print(messages['message'].loc[100],'\\n')\n",
    "print(messages['message'].loc[451])\n",
    "\n",
    "'''\n",
    "1. messages refers to the pandas DataFrame that contains the dataset.\n",
    "\n",
    "2. ['message'] selects the \"message\" column from the DataFrame,\n",
    "   which contains the SMS/text content.\n",
    "\n",
    "3. .loc[451] selects the row with index label 451.\n",
    "\n",
    "4. This returns the text message present at index 451 in the dataset.\n",
    "\n",
    "5. It is commonly used to inspect or view a specific sample message\n",
    "   from the corpus for understanding or debugging.\n",
    "''';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381ea8ef",
   "metadata": {},
   "source": [
    "### **Step-1: Text Preprocessing** \n",
    "    1. Tokenization, \n",
    "    2. StopWords, \n",
    "    3. Stemmming, \n",
    "    4. Lemmatization, \n",
    "    5. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96864c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and preprocessing\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b8a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer # See the Capital letters\n",
    "\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f49c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) # List to Set\n",
    "'''\n",
    "The final corpus contains all messages after:\n",
    "    - Cleaning\n",
    "    - Lowercasing\n",
    "    - Stopword removal\n",
    "    - Stemming\n",
    "'''\n",
    "\n",
    "corpus = [] # Group of Sentences\n",
    "for i in range(len(messages)): # for each sentence in that message\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['message'][i])\n",
    "    review = review.lower().split()\n",
    "\n",
    "    cleaned_words = [] # Group of Individual Words\n",
    "    for word in review: # for each word in that sentence\n",
    "        if word not in stop_words:\n",
    "            s_word = ps.stem(word)\n",
    "            cleaned_words.append(s_word) # Adds the Stemmed word to the Sentence List\n",
    "\n",
    "    Sentence = ' '.join(cleaned_words) # Forming the sentance from Cleaned Words\n",
    "    corpus.append(Sentence) # Adds the cleaned sentence to the corpus List \n",
    "\n",
    "corpus\n",
    "\n",
    "'''\n",
    "1. corpus = [] initializes an empty list to store the cleaned and processed text data.\n",
    "\n",
    "2. stop_words stores the set of English stopwords from NLTK,\n",
    "   which will be removed from the text during preprocessing.\n",
    "\n",
    "3. The for loop iterates over each message in the dataset.\n",
    "\n",
    "4. re.sub('[^a-zA-Z]', ' ', messages['message'][i])\n",
    "   - Replaces all non-alphabetic characters with spaces.\n",
    "   - Removes numbers, punctuation, and special symbols.\n",
    "\n",
    "5. review.lower().split()\n",
    "   - Converts text to lowercase.\n",
    "   - Splits the sentence into individual words (tokens).\n",
    "\n",
    "6. review1 = [] initializes a list to store processed words for the current message.\n",
    "\n",
    "7. For each word in review:\n",
    "   - If the word is not in stop_words,\n",
    "   - Apply Porter stemming using ps.stem(word),\n",
    "   - Append the stemmed word to review1.\n",
    "''';\n",
    "\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77a1d21",
   "metadata": {},
   "source": [
    "#### **Applying OHE for the y lable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f326f5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.get_dummies(messages['label']) # Dummies --> OHE.\n",
    "y = y.iloc[:,1] # selecting the 2nd column of the spam,ham\n",
    "\n",
    "'''\n",
    "0. get_dummies is a pandas function used to convert categorical values into numeric (one-hot encoded) columns.\n",
    "\n",
    "1. pd.get_dummies(messages['label']) converts the text labels (like \"ham\" and \"spam\")\n",
    "   into separate binary columns (one-hot encoding).\n",
    "\n",
    "2. Each label becomes a column with values 0 or 1.\n",
    "   Example:\n",
    "      \"ham\"  â†’ [1, 0]\n",
    "      \"spam\" â†’ [0, 1]\n",
    "\n",
    "3. y.iloc[:, 1] selects the second column from the dummy DataFrame,\n",
    "   which usually corresponds to the \"spam\" class.\n",
    "\n",
    "4. .values converts the selected column into a NumPy array.\n",
    "\n",
    "5. The final target vector y becomes:\n",
    "      spam â†’ 1\n",
    "      ham  â†’ 0\n",
    "\n",
    "6. This numeric y is used as the output/label variable\n",
    "   for training the machine learning classification model.\n",
    "'''\n",
    "\n",
    "# y = y.astype(int)\n",
    "y.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f0852",
   "metadata": {},
   "source": [
    "### **Step-2: Text --> Vectors** \n",
    "    1.BoW, \n",
    "    2. TF-IDF, \n",
    "    3. Word2Vec, \n",
    "    4. AvgWord2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f29f20e",
   "metadata": {},
   "source": [
    "### **Vectorization: (BoW / TF-IDF)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d03f797",
   "metadata": {},
   "source": [
    "#### **1. Creating the Bag Of Words Model for Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ae8e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Using CountVectorizer for BoW Vectors\n",
    "cv = CountVectorizer(max_features = 2500) # Implement the Bag of Words\n",
    "# cv = CountVectorizer(max_features = 2500, binary = True, ngram_range = (1,2))\n",
    "X = cv.fit_transform(corpus)\n",
    "\n",
    "X = X.toarray()\n",
    "\n",
    "'''\n",
    "0. These below lines implement the Bag of Words (BoW) model on your text data.\n",
    "\n",
    "1. CountVectorizer is a tool from scikit-learn that converts raw text into \n",
    "numerical feature vectors using the Bag of Words (BoW) model.\n",
    "\n",
    "2. This creates a CountVectorizer object.\n",
    "    What it does:\n",
    "    Converts text â†’ numeric vectors\n",
    "    Builds a vocabulary from your corpus\n",
    "    Represents each document by word counts\n",
    "\n",
    "3. Keeps only the top 2500 most frequent words in the corpus.\n",
    "\n",
    "4. x = cv.fit_transform(corpus)\n",
    "This does two things:\n",
    "    ðŸ”¹ fit(corpus)\n",
    "        Scans all documents in corpus\n",
    "        Learns the vocabulary (up to 2500 words)\n",
    "        Assigns each word an index (column)\n",
    "\n",
    "    ðŸ”¹ transform(corpus)\n",
    "        Converts each document into a vector\n",
    "        Each vector length = vocabulary size (â‰¤ 2500)\n",
    "        Values = word counts in that document\n",
    "'''\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cddff60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1db8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "spam_detect_model = MultinomialNB()\n",
    "spam_detect_model.fit(X_train, y_train)\n",
    "\n",
    "'''\n",
    "1. from sklearn.naive_bayes import MultinomialNB\n",
    "   - Imports the Multinomial Naive Bayes classifier from scikit-learn.\n",
    "   - This algorithm is commonly used for text classification problems\n",
    "     such as spam detection.\n",
    "\n",
    "2. MultinomialNB()\n",
    "   - Creates an instance of the Naive Bayes model.\n",
    "   - It is designed to work with discrete features like\n",
    "     word counts or TF-IDF values.\n",
    "\n",
    "3. .fit(X_train, y_train)\n",
    "   - Trains (fits) the model on the training data.\n",
    "   - X_train contains the feature vectors for the messages.\n",
    "   - y_train contains the corresponding labels\n",
    "     (e.g., 0 = ham, 1 = spam).\n",
    "\n",
    "4. spam_detect_model\n",
    "   - Stores the trained classifier.\n",
    "   - This model can now be used to make predictions on new data\n",
    "     using spam_detect_model.predict().\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8952ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "\n",
    "y_pred = spam_detect_model\n",
    "y_pred = y_pred.predict(X_test)\n",
    "\n",
    "'''\n",
    "1. spam_detect_model.predict(X_test)\n",
    "   - Uses the trained Naive Bayes model to make predictions.\n",
    "   - X_test contains the feature vectors of unseen/test messages.\n",
    "   - The model predicts the class label for each message.\n",
    "\n",
    "2. The result is stored in y_pred.\n",
    "   - y_pred is a NumPy array of predicted labels.\n",
    "   - Each value represents the predicted class:\n",
    "       0 â†’ ham (not spam)\n",
    "       1 â†’ spam\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4f8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracyScore = accuracy_score(y_test, y_pred)\n",
    "print(accuracyScore)\n",
    "\n",
    "'''\n",
    "1. from sklearn.metrics import accuracy_score\n",
    "   - Imports the accuracy_score function from scikit-learn.\n",
    "   - It is used to evaluate classification models.\n",
    "\n",
    "2. accuracy_score(y_test, y_pred)\n",
    "   - Compares the true labels (y_test) with the predicted labels (y_pred).\n",
    "   - Computes the accuracy as:\n",
    "       (Number of correct predictions) / (Total predictions)\n",
    "\n",
    "5. This accuracy indicates how well the spam detection model\n",
    "   correctly classifies messages as spam or ham.\n",
    "''';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4877f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classy_report = classification_report(y_test, y_pred)\n",
    "print(classy_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d66f597",
   "metadata": {},
   "source": [
    "#### **2. Creating the TF-IDF Model for Vectorization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ff40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Using TfidfVectorizer for TF-IDF Vectors\n",
    "tv = TfidfVectorizer(max_features = 2500)\n",
    "# tv = TfidfVectorizer(max_features = 2500, ngram_range = (1,2))\n",
    "X = tv.fit_transform(corpus)\n",
    "# X.toarray()\n",
    "# y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b0cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd760a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "spam_detect_model = MultinomialNB()\n",
    "spam_detect_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb4d8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = spam_detect_model\n",
    "y_pred = y_pred.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e671f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracyScore = accuracy_score(y_test,y_pred)\n",
    "print(accuracyScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ceeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classy_report = classification_report(y_test,y_pred)\n",
    "print(classy_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3a4838",
   "metadata": {},
   "source": [
    "#### **Sample Usage of \"RandomForestClassifier\" insted of \"NaiveBayes\"**\n",
    "\n",
    "#### **RandomForestClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e818511",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialising\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Training\n",
    "rf.fit(X_train, y_train) # not fit_transform\n",
    "\n",
    "# Predicting\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(acc)\n",
    "\n",
    "clss = classification_report(y_test, y_pred)\n",
    "print(clss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd86e6f1",
   "metadata": {},
   "source": [
    "### ðŸ§  **Word2Vec Implementation**\n",
    "\n",
    "Word2Vec is a technique to learn dense vector representations (embeddings) for words based on their context.\n",
    "\n",
    "It has **two main architectures**:\n",
    "1. **Skip-gram** â†’ predicts surrounding context words given a target word.  \n",
    "2. **CBOW (Continuous Bag of Words)** â†’ predicts the target word from surrounding context words.\n",
    "---\n",
    "1. **Pretrained Models**  \n",
    "   - Example: **Google News Word2Vec (300-dimensional vectors)**  \n",
    "   - Trained on massive corpora.  \n",
    "   - Ready to use and often give strong results.  \n",
    "\n",
    "2. **Fine-tuning / Training from Scratch**  \n",
    "   - Train Word2Vec on your own dataset.  \n",
    "   - Useful when domain-specific vocabulary is important  \n",
    "     (e.g., medical, legal, social media text).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050dcc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1eef02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
